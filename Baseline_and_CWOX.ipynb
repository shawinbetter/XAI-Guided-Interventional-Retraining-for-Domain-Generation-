{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b320e74b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40e0de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import torch\n",
    "import torchvision\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebda5b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.10.0\n",
      "CUDA Available: False\n",
      "# GPUS: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"# GPUS:\", torch.cuda.device_count())\n",
    "# for idx in range(torch.cuda.device_count()):\n",
    "#     print(idx, torch.cuda.get_device_name(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b2abf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration(cuda_device='cuda:1', imagenet_root='../datasets/ilsvrc2012/', imagenetv2_root='../datasets/imagenetv2-matched-frequency-format-val/', val_batch_size=512, val_loader_num_workers=4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class Configuration:\n",
    "    cuda_device: str = \"cuda:1\"\n",
    "    imagenet_root: str = \"../datasets/ilsvrc2012/\"\n",
    "    imagenetv2_root: str = \"../datasets/imagenetv2-matched-frequency-format-val/\"\n",
    "    val_batch_size: int = 512\n",
    "    val_loader_num_workers: int = 4\n",
    "\n",
    "        \n",
    "configuration = Configuration()\n",
    "configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c36eb6",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d799ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://pytorch.org/vision/stable/models.html#classification\n",
    "For res50,\n",
    "interpolation: bilinear\n",
    "input size: 224\n",
    "crop ratio: 0.85 (original -> 224/0.85 = 256, 224)\n",
    "\"\"\"\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(\n",
    "        int(math.floor(224 / 0.85)),\n",
    "        interpolation=torchvision.transforms.functional.InterpolationMode.BILINEAR\n",
    "    ),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb834068",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ImageNet (https://image-net.org)\n",
    "Download (after signup) from https://image-net.org/challenges/LSVRC/2012/2012-downloads.php\n",
    "- Development kit (Task 1 & 2). 2.5MB. ILSVRC2012_devkit_t12.tar.gz  \n",
    "- Validation images (all tasks). 6.3GB. MD5: 29b22e2961454d5413ddabcf34fc5622 ILSVRC2012_img_val.tar\n",
    "\"\"\"\n",
    "imagenet_training_dataset = torchvision.datasets.ImageNet(\n",
    "    root=configuration.imagenet_root, split=\"train\", transform=val_transform,\n",
    ")\n",
    "imagenet_validation_dataset = torchvision.datasets.ImageNet(\n",
    "    root=configuration.imagenet_root, split=\"val\", transform=val_transform,\n",
    ")\n",
    "\n",
    "imagenet_training_loader = torch.utils.data.DataLoader(\n",
    "    imagenet_training_dataset,\n",
    "    batch_size=configuration.val_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=configuration.val_loader_num_workers,\n",
    ")\n",
    "imagenet_validation_loader = torch.utils.data.DataLoader(\n",
    "    imagenet_validation_dataset,\n",
    "    batch_size=configuration.val_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=configuration.val_loader_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ImageNetV2 Matched Freq. (https://github.com/modestyachts/ImageNetV2)\n",
    "Download from http://imagenetv2public.s3-website-us-west-2.amazonaws.com/\n",
    "\"\"\"\n",
    "IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "\n",
    "class ImageNetV2(torchvision.datasets.folder.DatasetFolder):\n",
    "    \"\"\"\n",
    "    Modified from https://pytorch.org/vision/stable/_modules/torchvision/datasets/folder.html#ImageFolder\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            root=configuration.imagenetv2_root,\n",
    "            loader=torchvision.datasets.folder.default_loader,\n",
    "            extensions=IMG_EXTENSIONS,\n",
    "            transform=val_transform,\n",
    "        )\n",
    "        self.imgs = self.samples\n",
    "        \n",
    "    def find_classes(self, directory):\n",
    "        \"\"\"\n",
    "        By default, torchvision.datasets.folder.ImageFolder will sort the folder in the str type.\n",
    "        for example: \"0\", \"1\", \"10\", therefore we need a custom class_to_idx implementation here\n",
    "        \"\"\" \n",
    "        classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "        \n",
    "        class_to_idx = {cls_name: int(cls_name) for cls_name in classes}\n",
    "        return classes, class_to_idx\n",
    "        \n",
    "\n",
    "imagenetv2_dataset = ImageNetV2()\n",
    "\n",
    "imagenetv2_validation_loader = torch.utils.data.DataLoader(\n",
    "    imagenetv2_dataset,\n",
    "    batch_size=configuration.val_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=configuration.val_loader_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2272ec6",
   "metadata": {},
   "source": [
    "# Validate and store the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model = model.to(configuration.cuda_device).eval()\n",
    "    # Clean up the GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_labels = []\n",
    "        y_probas = []\n",
    "        for image, label in tqdm(dataloader):\n",
    "            image = image.to(configuration.cuda_device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                batch_logits = model(image)\n",
    "            \n",
    "            batch_probas = torch.nn.functional.softmax(batch_logits, dim=-1).cpu().numpy()\n",
    "            batch_labels = label.cpu().numpy()\n",
    "            y_probas.append(batch_probas)\n",
    "            y_labels.append(batch_labels)\n",
    "\n",
    "        y_labels = np.concatenate(y_labels)\n",
    "        y_probas = np.concatenate(y_probas)\n",
    "\n",
    "    print(f\"Top-1: {100 * top_k_accuracy_score(y_true=y_labels, y_score=y_probas, k=1): .3f}\")\n",
    "    print(f\"Top-5: {100 * top_k_accuracy_score(y_true=y_labels, y_score=y_probas, k=5): .3f}\")\n",
    "    \n",
    "    return y_probas, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4401de-7702-46a0-9184-df0727381962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_resnet50 = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903dee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# https://pytorch.org/vision/stable/models.html#classification\n",
    "# Top-1: 76.130 Top-5: 92.862\n",
    "# From the ImagenetV2 paper:\n",
    "# Top-1: 76.1 Top-5 92.9\n",
    "# \"\"\"\n",
    "\n",
    "probas, labels = evaluate(pretrained_resnet50, imagenet_validation_loader)\n",
    "np.save(\"./resnet50_imagenet_valid_probabilities.npy\", probas)\n",
    "np.save(\"./resnet50_imagenet_valid_labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a0231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# From the ImagenetV2 paper:\n",
    "# Top-1: 63.3 Top-5 84.7\n",
    "# \"\"\"\n",
    "\n",
    "probas, labels = evaluate(pretrained_resnet50, imagenetv2_validation_loader)\n",
    "np.save(\"./resnet50_imagenetv2_valid_probabilities.npy\", probas)\n",
    "np.save(\"./resnet50_imagenetv2_valid_labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae408b-a991-4d00-9685-ca9c4cd1ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probas, labels = evaluate(pretrained_resnet50, imagenet_training_loader)\n",
    "# np.save(\"./resnet50_imagenet_train_probabilities.npy\", probas)\n",
    "# np.save(\"./resnet50_imagenet_train_labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639dbf3-d3e8-4416-9441-40ba7fcf53b4",
   "metadata": {},
   "source": [
    "# Build the map from the label to its confusing/competing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7f368-1a3b-47d0-a777-7f9d35200382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained HLTM model\n",
    "from apply_hltm import apply_hltm\n",
    "import collections\n",
    "\n",
    "clusterify_resnet50 = apply_hltm(cut_level=0, json_path=\"./ResNet50.json\")\n",
    "cluster_cls_map = collections.defaultdict(set)\n",
    "for cls in range(0, 1000):\n",
    "    cluster_id = clusterify_resnet50.paths[cls][clusterify_resnet50.cut_depth]\n",
    "    cluster_cls_map[cluster_id].add(cls)\n",
    "\n",
    "def get_competing_classes(cls):\n",
    "    cluster_id = clusterify_resnet50.paths[cls][clusterify_resnet50.cut_depth]\n",
    "    return list(cluster_cls_map[cluster_id])\n",
    "\n",
    "print(get_competing_classes(0))\n",
    "print(get_competing_classes(389))\n",
    "print(get_competing_classes(402))\n",
    "print(get_competing_classes(889))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3786cf-d5e9-46d5-ac43-c1477c0b64fa",
   "metadata": {},
   "source": [
    "# Compute the GradCam for the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c174f-51ec-45a4-b5c3-5a4139b8aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchray.attribution.grad_cam import grad_cam\n",
    "from plt_wox import imsc\n",
    "\n",
    "def get_activation_heat_map(\n",
    "    model, probs, labels, dataset, batch_size, output_dir, layer_use=\"layer4\",\n",
    "\n",
    "):\n",
    "    probs = torch.Tensor(probs)\n",
    "    model.to(configuration.cuda_device)\n",
    "    \n",
    "    # Keep track of the original index\n",
    "    for processing_cls in tqdm(range(730, 1000)):\n",
    "        original_indices = []\n",
    "        all_saliency = []\n",
    "        # Clean up the GPU cache\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        # Get the competing classes\n",
    "        poss = get_competing_classes(processing_cls)\n",
    "\n",
    "        # Processing the images with the same predicted class\n",
    "        _indice = np.where(labels == processing_cls)[0]\n",
    "        print(f\"Processing {len(_indice)} images with predicted class: {processing_cls}.\")\n",
    "        original_indices += list(_indice)\n",
    "        \n",
    "        _probs = probs[_indice, :]\n",
    "        \n",
    "        # Fetch the coressponding images from the pytorch dataset\n",
    "        images = []\n",
    "        for idx in _indice:\n",
    "            images.append(dataset[idx][0])\n",
    "        images = torch.stack(images).to(configuration.cuda_device)\n",
    "\n",
    "        # Compute the gradcam\n",
    "        grad = torch.zeros_like(_probs)\n",
    "        poss_p = _probs[:, poss]\n",
    "        grad[:, poss] = poss_p / poss_p.sum()\n",
    "        grad = grad.to(configuration.cuda_device)\n",
    "        \n",
    "        # Batch processing\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while start < len(_indice):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                saliency = grad_cam(\n",
    "                    model, images[start: end, :], grad[start: end, :],\n",
    "                    saliency_layer=layer_use, resize = True,\n",
    "                )\n",
    "            saliency = saliency.detach().cpu().numpy().reshape(-1, 224, 224)\n",
    "            saliency = np.clip(saliency, a_min=0, a_max=None)\n",
    "            all_saliency.append(saliency)\n",
    "            start += batch_size\n",
    "            end += batch_size\n",
    "        \n",
    "        all_saliency = np.concatenate(all_saliency)\n",
    "        original_indices = np.array(original_indices)\n",
    "        np.save(f\"{output_dir}/class_{processing_cls}_original_indices.npy\", original_indices)\n",
    "        np.save(f\"{output_dir}/class_{processing_cls}_saliency.npy\", all_saliency)\n",
    "        print(f\"Save results to {output_dir}\")\n",
    "        print(all_saliency.shape, original_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac0868-6fb8-4d2f-9339-a9899518869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions from the baseline pretrained model\n",
    "train_probas = np.load(\"./resnet50_imagenet_train_probabilities.npy\")\n",
    "train_labels = np.load(\"./resnet50_imagenet_train_labels.npy\")\n",
    "get_activation_heat_map(\n",
    "    pretrained_resnet50,\n",
    "    train_probas, train_labels, imagenet_training_loader.dataset,\n",
    "    output_dir = \"./competiting_classes_gradcam\",\n",
    "    batch_size = configuration.val_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8f0f0-d538-4cb1-b01f-de1e5b09a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cls in [0, 486, 889, 402, 546]:\n",
    "    print(\"Class\", cls)\n",
    "    saliency = np.load(f\"./competiting_classes_gradcam/class_{cls}_saliency.npy\")\n",
    "    original_indices = np.load(f\"./competiting_classes_gradcam/class_{cls}_original_indices.npy\")\n",
    "    \n",
    "    for idx in [0, 100, 500, 1000, 1200]:\n",
    "        img=imsc(imagenet_training_loader.dataset[original_indices[idx]][0])\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(saliency[idx], cmap='jet', alpha=0.6)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e2c24-cc9e-4556-b0ed-887a49b575f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
